import google.generativeai as genai
from utils.yaml_loader import load_yaml_settings

def get_gemini_model(model_name="models/gemini-1.5-flash"):
    settings = load_yaml_settings()
    api_key = settings.get("gemini_api_key")
    if not api_key:
        raise ValueError("GEMINI_API_KEY not found in settings.yaml")
    genai.configure(api_key=api_key)
    return genai.GenerativeModel(model_name)

def refine_translation(chinese_text: str, raw_translation: str) -> str:
    model = get_gemini_model()
    prompt = f"""
B·∫°n l√† m·ªôt bi√™n t·∫≠p vi√™n ti·∫øng Vi·ªát chuy√™n ch·ªânh s·ª≠a b·∫£n d·ªãch truy·ªán ti√™n hi·ªáp t·ª´ ti·∫øng Trung.

D∆∞·ªõi ƒë√¢y l√† m·ªôt ƒëo·∫°n vƒÉn g·ªëc ti·∫øng Trung v√† b·∫£n d·ªãch m√°y sang ti·∫øng Vi·ªát. H√£y:
- S·ª≠a c√°c c√¢u d·ªãch sai nghƒ©a, c·ª©ng nh·∫Øc ho·∫∑c kh√¥ng t·ª± nhi√™n.
- D·ªãch l·∫°i c√°c t·ª´/ƒëo·∫°n ti·∫øng Trung c√≤n s√≥t l·∫°i ch∆∞a ƒë∆∞·ª£c d·ªãch (n·∫øu c√≥).
- Gi·ªØ vƒÉn phong truy·ªán ti√™n hi·ªáp, kh√¥ng th√™m ch√∫ th√≠ch ho·∫∑c nh·∫≠n x√©t.
- Tr·∫£ v·ªÅ b·∫£n d·ªãch ƒë√£ hi·ªáu ƒë√≠nh ho√†n ch·ªânh.

### VƒÉn b·∫£n g·ªëc (ti·∫øng Trung g·ªëc):
{chinese_text}

### B·∫£n d·ªãch m√°y:
{raw_translation}

### B·∫£n d·ªãch sau khi hi·ªáu ƒë√≠nh:
"""
    try:
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        print(f"C√≥ l·ªói x·∫£y ra khi d·ªãch vƒÉn b·∫£n: {e}")
        return ""


# ‚ú® H√ÄM T√ìM T·∫ÆT VƒÇN B·∫¢N (kho·∫£ng 15% n·ªôi dung g·ªëc)
def summarize_text(text: str) -> str:
    model = get_gemini_model()
    prompt = f"""
T√¥i c√≥ m·ªôt ƒëo·∫°n truy·ªán sau. H√£y t√≥m t·∫Øt n√≥ v·ªõi ƒë·ªô d√†i kho·∫£ng 20% ƒë·ªô d√†i g·ªëc. Ch·ªâ ghi n·ªôi dung ch√≠nh v√† ng·∫Øn g·ªçn, gi·ªØ m·∫°ch truy·ªán li·ªÅn l·∫°c, kh√¥ng th√™m nh·∫≠n x√©t hay ch√∫ th√≠ch.

### N·ªôi dung:
{text}

### T√≥m t·∫Øt:
"""
    try:
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        print(f"L·ªói khi t√≥m t·∫Øt vƒÉn b·∫£n: {e}")
        return ""

# üìö H√ÄM T√ìM T·∫ÆT TO√ÄN B·ªò - ƒê·ªÜ QUY NHI·ªÄU B∆Ø·ªöC
def recursive_summary(summaries: list[str], max_chunk_size=5) -> str:
    """
    Nh·∫≠n v√†o danh s√°ch c√°c b·∫£n t√≥m t·∫Øt ch∆∞∆°ng, t√≥m g·ªçn ch√∫ng th√†nh m·ªôt b·∫£n t√≥m t·∫Øt cu·ªëi c√πng.
    Qu√° tr√¨nh t√≥m t·∫Øt nhi·ªÅu b∆∞·ªõc n·∫øu c√≥ > max_chunk_size ph·∫ßn.
    """
    if not summaries:
        return ""

    model = get_gemini_model()

    # N·∫øu s·ªë l∆∞·ª£ng nh·ªè h∆°n max_chunk_size, g·ªôp v√† t√≥m t·∫Øt 1 l·∫ßn duy nh·∫•t
    if len(summaries) <= max_chunk_size:
        prompt = f"""
T√¥i c√≥ m·ªôt s·ªë ƒëo·∫°n t√≥m t·∫Øt truy·ªán. H√£y g·ªôp n·ªôi dung v√† t√≥m t·∫Øt th√†nh m·ªôt ƒëo·∫°n g·ªçn g√†ng, ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu. Kh√¥ng th√™m b√¨nh lu·∫≠n hay ti√™u ƒë·ªÅ.

### C√°c t√≥m t·∫Øt ch∆∞∆°ng:
{'\n\n'.join(summaries)}

### T√≥m t·∫Øt cu·ªëi c√πng:
"""
        try:
            response = model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            print(f"L·ªói khi t√≥m t·∫Øt t·∫•t c·∫£: {e}")
            return ""

    # N·∫øu d√†i h∆°n, chia nh·ªè r·ªìi t√≥m t·∫Øt t·ª´ng nh√≥m
    print(f"üîÅ T√≥m t·∫Øt theo nh√≥m {max_chunk_size}...")
    grouped_summaries = [
        summaries[i:i + max_chunk_size]
        for i in range(0, len(summaries), max_chunk_size)
    ]

    intermediate_summaries = []
    for idx, group in enumerate(grouped_summaries):
        print(f"üìö ƒêang t√≥m t·∫Øt nh√≥m {idx + 1}/{len(grouped_summaries)}...")
        group_summary = recursive_summary(group, max_chunk_size)
        intermediate_summaries.append(group_summary)

    # ƒê·ªá quy ti·∫øp t·ª•c tr√™n c√°c b·∫£n t√≥m t·∫Øt trung gian
    return recursive_summary(intermediate_summaries, max_chunk_size)
